{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用于特征提取和图像匹配变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sobel(image):\n",
    "    # 使用Sobel滤波器计算水平和垂直梯度\n",
    "    sobelx = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    sobely = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    # 合并水平和垂直梯度\n",
    "    gradient_magnitude = np.sqrt(sobelx**2 + sobely**2)\n",
    "    return gradient_magnitude\n",
    "\n",
    "# 1. 特征提取\n",
    "def extract_features(image):\n",
    "    # # 使用Sobel滤波器计算水平和垂直梯度\n",
    "    # sobelx = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    # sobely = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    # # 合并水平和垂直梯度\n",
    "    # gradient_magnitude = np.sqrt(sobelx**2 + sobely**2)\n",
    "    # gradient_magnitude = cv2.convertScaleAbs(gradient_magnitude)  # 转换为8位无符号整数图像\n",
    "    feature_extractor = cv2.SIFT_create()\n",
    "    keypoints, descriptors = feature_extractor.detectAndCompute(image, None)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "\n",
    "# def extract_features_orb(image):\n",
    "#     orb = cv2.ORB_create(nfeatures=3000)\n",
    "#     keypoints, descriptors = orb.detectAndCompute(image, None)\n",
    "#     if descriptors is not None:\n",
    "#         descriptors = descriptors.astype(np.float32)\n",
    "#     else:\n",
    "#         descriptors = np.array([])  # 或者设置一个默认的空描述符\n",
    "#     return keypoints, descriptors\n",
    "\n",
    "# 2. 特征匹配\n",
    "def match_features_bf(descriptors1, descriptors2):\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.5 * n.distance:\n",
    "            good_matches.append(m)\n",
    "    return good_matches\n",
    "\n",
    "def match_features_flann(descriptors1, descriptors2):\n",
    "    # FLANN 参数设置\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_params = dict(checks=50)\n",
    "    # 创建 FLANN 匹配器\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    # 使用 KNN 进行匹配\n",
    "    matches = flann.knnMatch(descriptors1, descriptors2, k=2)\n",
    "    # 选择好的匹配\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.5 * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    return good_matches\n",
    "\n",
    "# 3. 变换估计\n",
    "def estimate_transform(keypoints1, keypoints2, good_matches):\n",
    "    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    # 使用RANSAC算法估计变换矩阵\n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "    return M\n",
    "\n",
    "#4. 变换应用\n",
    "def apply_transform(image, M):\n",
    "    # 应用估计的变换矩阵到图像\n",
    "    transformed_image = cv2.warpPerspective(image, M, (image.shape[1], image.shape[0]))\n",
    "    return transformed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一次配准\n",
    "image1 = cv2.imread('origin_images/442.png', cv2.IMREAD_GRAYSCALE)\n",
    "image2 = cv2.imread('origin_images/222.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# 1. 特征提取\n",
    "keypoints1, descriptors1 = extract_features(image1)\n",
    "keypoints2, descriptors2 = extract_features(image2)\n",
    "\n",
    "# 2. 特征匹配\n",
    "good_matches = match_features_flann(descriptors1, descriptors2)\n",
    "\n",
    "# 3. 变换估计\n",
    "M = estimate_transform(keypoints1, keypoints2, good_matches)\n",
    "\n",
    "# 4. 变换应用\n",
    "transformed_image = apply_transform(image1, M)\n",
    "cv2.imwrite('trans_image/transformed442to222.png', transformed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 二次配准\n",
    "# image11 = transformed_image\n",
    "# # 1. 特征提取\n",
    "# keypoints11, descriptors11 = extract_features(image11)\n",
    "# # 2. 特征匹配\n",
    "# good_matches2 =match_features(descriptors11, descriptors2)\n",
    "# # 3. 变换估计\n",
    "# M = estimate_transform(keypoints11, keypoints2, good_matches2)\n",
    "# # 4. 变换应用\n",
    "# transformed_image = apply_transform(image11, M)\n",
    "# cv2.imwrite('2transformed2to222.png', transformed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 后面是SIFT和BFMatch合理性判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一次配准\n",
    "# 1. 特征提取\n",
    "keypoints1, descriptors1 = extract_features(image1)\n",
    "keypoints2, descriptors2 = extract_features(image2)\n",
    "\n",
    "img3 = cv2.drawKeypoints(image1,keypoints1,image1,color=(255,0,255))\n",
    "img4 = cv2.drawKeypoints(image2,keypoints2,image2,color=(255,0,255))\n",
    "hmerge = np.hstack((img3, img4))\n",
    "# pillow_image = Image.fromarray(cv2.cvtColor(hmerge, cv2.COLOR_BGR2RGB))\n",
    "# pillow_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BFMatcher解决匹配\n",
    "#可能还得试试其他的匹配方法和\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(descriptors1,descriptors2, k=2)\n",
    "#print(matches)\n",
    "#print(len(matches))\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.5*n.distance:\n",
    "        good.append([m])\n",
    "img5 = cv2.drawMatchesKnn(image1,keypoints1,image2,keypoints2,matches,None,flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS | cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "pillow_image = Image.fromarray(cv2.cvtColor(img5, cv2.COLOR_BGR2RGB))\n",
    "pillow_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img6 = cv2.drawMatchesKnn(image1,keypoints1,image2,keypoints2,good,None,flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS | cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "pillow_image = Image.fromarray(cv2.cvtColor(img6, cv2.COLOR_BGR2RGB))\n",
    "pillow_image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
