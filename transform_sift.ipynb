{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用于特征提取和图像匹配变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sobel(image):\n",
    "    # 使用Sobel滤波器计算水平和垂直梯度\n",
    "    sobelx = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    sobely = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    # 合并水平和垂直梯度\n",
    "    gradient_magnitude = np.sqrt(sobelx**2 + sobely**2)\n",
    "    return gradient_magnitude\n",
    "\n",
    "# 1. 特征提取\n",
    "def extract_features(image):\n",
    "    # image_array = np.array(image)\n",
    "    feature_extractor = cv2.SIFT_create()\n",
    "    keypoints, descriptors = feature_extractor.detectAndCompute(image, None)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "# 2. 特征匹配\n",
    "def match_features_bf(descriptors1, descriptors2):\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(descriptors1, descriptors2, k=2)\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.5 * n.distance:\n",
    "            good_matches.append(m)\n",
    "    return good_matches\n",
    "\n",
    "def match_features_flann(descriptors1, descriptors2):\n",
    "    # FLANN 参数设置\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_params = dict(checks=50)\n",
    "    # 创建 FLANN 匹配器\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    # 使用 KNN 进行匹配\n",
    "    matches = flann.knnMatch(descriptors1, descriptors2, k=2)\n",
    "    # 选择好的匹配\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.5 * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    return good_matches\n",
    "# 注掉的方法是基于透视变换\n",
    "# # 3. 变换估计\n",
    "# def estimate_transform(keypoints1, keypoints2, good_matches):\n",
    "#     src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "#     dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "#     # 使用RANSAC算法估计变换矩阵\n",
    "#     M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "#     return M\n",
    "\n",
    "# #4. 变换应用\n",
    "# def apply_transform(image, M):\n",
    "#     # 应用估计的变换矩阵到图像\n",
    "#     transformed_image = cv2.warpPerspective(image, M, (image.shape[1], image.shape[0]))\n",
    "#     return transformed_image\n",
    "\n",
    "# 3. 仿射变换估计\n",
    "def estimate_transform(keypoints1, keypoints2, good_matches):\n",
    "    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    # 使用RANSAC算法估计仿射变换矩阵\n",
    "    M = cv2.estimateAffinePartial2D(src_pts, dst_pts)[0]\n",
    "    return M\n",
    "\n",
    "# 4. 仿射变换应用\n",
    "def apply_transform(image, M):\n",
    "    # 应用估计的仿射变换矩阵到图像\n",
    "    # image = Image.fromarray(image)\n",
    "    transformed_image = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]))\n",
    "    return transformed_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\features2d\\src\\sift.dispatch.cpp:512: error: (-5:Bad argument) image is empty or has incorrect depth (!=CV_8U) in function 'cv::SIFT_Impl::detectAndCompute'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32md:\\桌面\\Image_registration\\transform_sift.ipynb 单元格 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m image2 \u001b[39m=\u001b[39m image2\u001b[39m/\u001b[39marray\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# image1 = np.clip(image1*255,0,255).astype(np.uint8)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# image2 = np.clip(image2*255,0,255).astype(np.uint8)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# image1 = Image.fromarray(img1)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# image2 = Image.fromarray(img2)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# 1. 特征提取\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m keypoints1, descriptors1 \u001b[39m=\u001b[39m extract_features(image1)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m keypoints2, descriptors2 \u001b[39m=\u001b[39m extract_features(image2)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# 2. 特征匹配\u001b[39;00m\n",
      "\u001b[1;32md:\\桌面\\Image_registration\\transform_sift.ipynb 单元格 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_features\u001b[39m(image):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# image_array = np.array(image)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     feature_extractor \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mSIFT_create()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     keypoints, descriptors \u001b[39m=\u001b[39m feature_extractor\u001b[39m.\u001b[39;49mdetectAndCompute(image, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/Image_registration/transform_sift.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m keypoints, descriptors\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\features2d\\src\\sift.dispatch.cpp:512: error: (-5:Bad argument) image is empty or has incorrect depth (!=CV_8U) in function 'cv::SIFT_Impl::detectAndCompute'\n"
     ]
    }
   ],
   "source": [
    "# 一次配准\n",
    "image1 = cv2.imread('origin_images/2.png', cv2.IMREAD_GRAYSCALE).astype(np.float64)\n",
    "image2 = cv2.imread('origin_images/222.png', cv2.IMREAD_GRAYSCALE).astype(np.float64)\n",
    "\n",
    "array = np.load('anti.npy')\n",
    "array = Image.fromarray(array)\n",
    "image1 = image1/array\n",
    "image2 = image2/array\n",
    "image1 = np.clip(image1*255,0,255).astype(np.uint8)\n",
    "image2 = np.clip(image2*255,0,255).astype(np.uint8)\n",
    "# image1 = Image.fromarray(img1)\n",
    "# image2 = Image.fromarray(img2)\n",
    "\n",
    "# 1. 特征提取\n",
    "keypoints1, descriptors1 = extract_features(image1)\n",
    "keypoints2, descriptors2 = extract_features(image2)\n",
    "\n",
    "# 2. 特征匹配\n",
    "good_matches = match_features_flann(descriptors1, descriptors2)\n",
    "\n",
    "# 3. 变换估计\n",
    "M = estimate_transform(keypoints1, keypoints2, good_matches)\n",
    "\n",
    "# 4. 变换应用\n",
    "transformed_image = apply_transform(image1, M)\n",
    "cv2.imwrite('trans_image/transformed2to222.png', transformed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 二次配准\n",
    "# image11 = transformed_image\n",
    "# # 1. 特征提取\n",
    "# keypoints11, descriptors11 = extract_features(image11)\n",
    "# # 2. 特征匹配\n",
    "# good_matches2 =match_features(descriptors11, descriptors2)\n",
    "# # 3. 变换估计\n",
    "# M = estimate_transform(keypoints11, keypoints2, good_matches2)\n",
    "# # 4. 变换应用\n",
    "# transformed_image = apply_transform(image11, M)\n",
    "# cv2.imwrite('2transformed2to222.png', transformed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 后面是SIFT和BFMatch合理性判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一次配准\n",
    "# 1. 特征提取\n",
    "keypoints1, descriptors1 = extract_features(image1)\n",
    "keypoints2, descriptors2 = extract_features(image2)\n",
    "\n",
    "img3 = cv2.drawKeypoints(image1,keypoints1,image1,color=(255,0,255))\n",
    "img4 = cv2.drawKeypoints(image2,keypoints2,image2,color=(255,0,255))\n",
    "hmerge = np.hstack((img3, img4))\n",
    "# pillow_image = Image.fromarray(cv2.cvtColor(hmerge, cv2.COLOR_BGR2RGB))\n",
    "# pillow_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BFMatcher解决匹配\n",
    "#可能还得试试其他的匹配方法和\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(descriptors1,descriptors2, k=2)\n",
    "#print(matches)\n",
    "#print(len(matches))\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.5*n.distance:\n",
    "        good.append([m])\n",
    "img5 = cv2.drawMatchesKnn(image1,keypoints1,image2,keypoints2,matches,None,flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS | cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "pillow_image = Image.fromarray(cv2.cvtColor(img5, cv2.COLOR_BGR2RGB))\n",
    "pillow_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img6 = cv2.drawMatchesKnn(image1,keypoints1,image2,keypoints2,good,None,flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS | cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "pillow_image = Image.fromarray(cv2.cvtColor(img6, cv2.COLOR_BGR2RGB))\n",
    "pillow_image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
